name: Archive Daily CSV

permissions:
  contents: write

on:
  schedule:
    - cron: '0 8 * * *'
  workflow_dispatch:

jobs:
  archive-csv:
    runs-on: ubuntu-latest

    steps:
      # 1) Check out ALL branches so we have both main and data-archive
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0            # grab full history
          persist-credentials: true  # allow pushing later

      # 2) Set up Python & install deps
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # 3) Run the scraper from the MAIN branchâ€™s code
      - name: Run scraper
        run: python scraper/daily_scrape.py
      
        # 4) Save the fresh CSV aside (before switching branches)
      - name: Save aside new_articles.csv
        run: |
          mkdir -p tmp
          cp data/new_articles.csv tmp/new_articles.csv

      # 5) Switch to data-archive branch and restore the CSV
      - name: Switch to data-archive branch
        run: |
          # Define the date first
          DATE=$(date -u +'%Y-%m-%d')
          git fetch origin data-archive:data-archive
          git checkout data-archive

          # Now put the fresh file into archives/DATE.csv
          mkdir -p archives
          cp tmp/new_articles.csv archives/${DATE}.csv

      # 6) Commit only the new CSV file
      - name: Commit & push new CSV
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          DATE=$(date -u +'%Y-%m-%d')
          git add archives/${DATE}.csv

          # Commit just those two paths
          git commit -m "Archive CSV for ${DATE}" \
            -- archives/${DATE}.csv
          git push origin data-archive
