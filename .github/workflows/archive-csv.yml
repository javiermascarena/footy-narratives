name: Archive Daily CSV

permissions:
  contents: write

on:
  schedule:
    - cron: '0 8 * * *'
  workflow_dispatch:

jobs:
  archive-csv:
    runs-on: ubuntu-latest

    steps:
      # 1) Check out ALL branches so we have both main and data-archive
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0            # grab full history
          persist-credentials: true  # allow pushing later

      # 2) Set up Python & install deps
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # 3) Run the scraper from the MAIN branch’s code
      - name: Run scraper
        run: python scraper/daily_scrape.py

      # 4) Switch to data-archive branch (creating it if it doesn’t exist locally)
      - name: Switch to data-archive branch
        run: |
          git fetch origin data-archive:data-archive
          git checkout data-archive

      # 5) Commit only the new CSV file
      - name: Commit & push new CSV
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Archive it by date
          DATE=$(date -u +'%Y-%m-%d')
          mkdir -p archives
          cp data/new_articles.csv archives/${DATE}.csv
          git add archives/${DATE}.csv

          # Commit just those two paths
          git commit -m "Archive CSV for ${DATE}" \
            -- archives/${DATE}.csv

          git push origin data-archive
